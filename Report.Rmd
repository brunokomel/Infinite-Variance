---
title: "Report"
author: "Derl Clausen, Bruno Komel, Rakeen Tanvir"
date: "5/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# TD
Rakeen
1. Compare different price variables (?) including volume vs. first difference. (In progress, is it helpful?)
2. [Bootstrap] Sampling distribution from our data and find a model that fits the data and use it to estimate the population variance (check if this can be estimated using bootstrapping). Variance increases as sample size increases? (Find in lecture, notes from Alek). Barplot, pdf overlaid on histogram. Compare classical (CLT, chi-square) vs. simulation (permutation), which works better? Leverage theoretical knowledge. (In Progress)
3.  Read https://stats.stackexchange.com/questions/2504/test-for-finite-variance, ecdf (DONE)
4. Integration on infinite variance to demonstrate divergence. Select data to model with a continuous distribution (exponential, gamma, chi square, beta). Dispaly a histogram of your data and overlay on it a graph of the density function for your hypothesized distribution. Then use integration is R to compute the expected number of counts in various bins, and compare with the observed number of counts. (Bruno?)
Bruno here: What are you thinking when you say "use integration to compute the expected number of counts in the bins"? 
Rakeen: so instead of using the built-in CDF function we just manually integrate over the density function to get the same expected counts that you got already. This is really just to demonstrate that we can do integration in R. 


Derl
1. [X]Test if data is well modeled by random walk with fractal analysis (https://blog.quantinsti.com/hurst-exponent/). 
  What would infinite variance in economic price indices mean for the long-term future of humanity? Does the stock market also have infinite growth? Political party/regime? When does it all end? What about automation? What is the optimization curve look like? 
2. []Analysis of contingency table with recession by more granular than annual period. Repeat chi-square test by party, and also repeat for regime. (Surprising that Democratic regimes bore brunt of recession(s) but have higher first differences and less recession time periods. Interesting if recession seems to be more associated with one categorical response than another, that we have so much variance, reflecting a random walk. Regime, recession, cycles, Fourier, random walk.)
3. [X]Fourier analysis, signal processing on First Difference Series. 
4. []Report, Overleaf (include effect of policy write-up), Video
5. []Hurst Exponent vs time. Different time frames within DJI. Run the same analysis on the two other big boy indices (S&P 500, NASDAQ)

Bruno
1. Test Opens and diffs for normal, Cauchy, Pareto, stable/Levy, student's t. Barplot, pdf overlaid on histogram. Use bins, quantiles to compare distributions. 
2. Linear and multiple regression analysis to predict values by regime and party. (keep aberrant data but disregard events?, boolean variable, e.g. coronavirus, black monday). Logistic regression analysis on probabilities for recession, party. Include covariance and correlation calculations, Rsquared. 
3. What is the standard deviation of the COVID-19 crash? Its significance? How many 5sigma events do we have in the data? 2007-8 market crash?
 - I added some stuff on this. There are 32 days where the flux was greater than 5 sigma. How do we want to discuss this?

# Additional, ideal list of comprehensive data analysis workflows I'd like to include:
1. Relevant data and case studies for introduction of topic and discussion of results. 
2. Exploratory data analysis with numeric summaries including center, spread and shape, novel statistics (e.g. trimmed mean, maximum or minimum, skewness, ratios).
Basic plots including boxplot, normal quantile plot, empirical cumulative distribution function and scatter plot. Nice pretty 3D plot (time vs. price vs. variance) or other plots yet unused in class used ggplot functions. 
3. Hypothesis testing with and without permutation tests. 
4. Demonstrate sampling distributions, central limit theorem (CLT) for binomial data, continuity correction for discrete random variables, and CLT for sampling without replacement. 
5. Bootstrapping to demonstrate the plug-in principle to estimate the population mean, bootstrapping percentile intervals, two-sample bootstrap including matched pairs, estimation of bias, monte carlo sampling (the second boostrap principle), sample mean with both large and small sample sizes, sample median and the mean-variance relationship.
6. Estimation. (Maximum Likelihood Estimation)
7. Confidence intervals with and without bootstrapping.
8. Regression.
9. Categorical data analysis.
10. Bayesian methods.
11. One-way ANOVA.
12. Distributions. Multivariate/bivariate normal distribution with covariance matrix and correlation coefficients. Check out what kind of data (or manipulation) can make chi-square, gamma or beta distribution and theory relevant. 

# The points below are broken into required and additional points. To achieve full credit, you must do all of the required points, and a subset of 10 of the additional points. To facilitate grading, leave an index/comment in your long script identifying where/how you think you earned your points within your analysis.

## Required dataset standards (DONE)
1. A dataframe. (DJI)
2. At least two categorical or logical columns. (political regime, year of expansion vs. recession)
3. At least two numeric columns. (open, close, high, low, volume, diffs)
4. At least 20 rows, preferably more, but real-world data may be limited. (8858 rows)

## Required graphical displays (all graphs must be colored and nicely labeled) (TD)
1. A barplot. (with fitdistr model overlaid on barplot of data) (suggestions herE??)
2. A histogram. (DONE)
3. A probability density graph overlaid on a histogram. (fitdistr) (DONE)
4. A contingency table. (DONE)

## Required analysis (TD)
1. A permutation test. (regime change, president permutation tests, effect of policy)
2. A p-value or other statistic based on a distribution function.
3. Analysis of a contingency table.
4. Comparison of analysis by classical methods (chi-square, CLT) and simulation methods. (set seed in R, if infinite variance then this can be seen with more simulations and fat tails)

## Required submission uploads (DONE)
1. A .csv file with the dataset (DJI)
2. A long, well-commented script that loads the dataset, explores it, and does all the analysis. (Main.R file)
3. A shorter .Rmd with compiled .pdf or .html file that presents highlights in ten minutes. (Report.Rmd file)
4. A one-page handout that explains the dataset and summarizes the analysis. (https://www.overleaf.com/project/5e7665484b0d3600011e16d0)


## Additional points for creativity or complexity (You may attempt as many as you like, for a maximum possible of 10 points)
1. A data set with lots of columns, allowing comparison of many different variables. (by volume, decade, year, month) (Recession by Party and Regime)
2. A data set that is so large that it can be used as a population from which samples are taken. (35 x ~365 rows) (Bootstrap sampling distribution)
3. A one-page document that discusses ethical issues related to collection of the dataset. (N/A)
4. A one-page document that discusses ethical issues raised by conclusions reached from analysis of the data. (What would infinite variance in economic price indices mean for the long-term future of humanity?)
5. A graphical display that is different from those in the textbook or in the class scripts.
6. Appropriate use of R functions for a probability distribution other than binomial, normal, or chi-square. (Random walk model, possibly also Cauchy, Levy, Pareto, T)
7. Appropriate use of integration to calculate a significant result.
8. A convincing demonstration of a relationship that might not have been statistically significant but that turns out to be so.
9. A convincing demonstration of a relationship that might have been statistically significant but that turns out not to be so.
10. Professional-looking software engineering (e.g defining and using your own functions).
11. Nicely labeled graphics using ggplot, with good use of color, line styles, etc., that tell a convincing story. 
12. An example where permutation tests or other computational techniques clearly work better than classical methods. (?)
13. Appropriate use of novel statistics (e.g. trimmed mean, maximum or minimum, skewness, ratios).
14. Use of linear regression. (keep aberrant data but disregard events?, boolean variable, e.g. coronavirus, black monday)
15. Calculation and display of a logistic regression curve. (?)
16. Appropriate use of covariance or correlation. (?)
17. Use of theoretical knowledge of chi-square, gamma, or beta distributions.
18. Use of theoretical knowledge of sampling distributions. (variance)
19. A graphical display that is different from those in the class scripts.
20. Calculation of a confidence interval. 
21. Appropriate use of quantiles to compare distributions. (random walk bins, quantiles)
22. Team consists of exactly two members (otherwise, 1 or 3 is a possibility). (3)
23. A video of the short script is posted on YouTube and a link to it is left in your long script. 