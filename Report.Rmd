---
title: "Report"
author: "Derl Clausen, Bruno Komel, Rakeen Tanvir"
date: "5/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<<<<<<< HEAD
#Outline
Exploratory Data Analysis
Pareto Distribution
Linear and Logistic Regression
Fourier Analysis
Random Walk
Fractal Analysis
Cauchy Distribution
Bootstrap
Partial Variance
Divergent Integration
Stable Distribution
Kolmogorov-Smirnov Test
Chi-square Test on Varying Time Scales
The Impact of Political Regimes - Hypothesis Testing With Permutation Test 
Hypothesis Testing: Contingency table with chi-square test
=======

<<<<<<< HEAD
<<<<<<< HEAD
#Investigating Interesting Correlations - Regressions
An interesting correlation that we would not have expected to be significant, but turns out to be so is that
of Open prices and Volume:
```{r,echo=T}
plot(log(Open)~log(Volume)) 
```
This looks fairly  promising. 

Compare logarithms of open price and trade volume variables
```{r,echo=TRUE}
x <- log(Volume)
y <- log(Open)

#Let us first manually run the regression, and then we'll check our results against the built-in
#function. First, let's find the slope of the line
b <- sum( (x-mean(x))*(y-mean(y)) / sum((x-mean(x))^2));b 
#Alternative - works because division by n-1 cancels out
cov(x,y)/var(x)
#Here is the formula for the intercept
a <- mean(y) - b*mean(x);a    
#It is quicker to use the built-in R function
linmod <- lm(y~x)
linmod; a; b #And we get the same result
plot(y~x, main = "Plot of First Differences by Volume", xlab = "log(Volume)", ylab = "log(Absolute First Differences)")
abline(a, b, col = "cyan")
abline(linmod$coefficients[1], linmod$coefficients[2], col = "red")
summary(linmod) 
```
And we get an R-squared that is 0.7298, so the linear model explains 73% of the data, and it appears that volume and open prices are positively correlated.

#The Impact of "Rare" Events
Let's shift now to taking a look at presumably rare events, and how they are impacting our results:
Let's consider each of the days in our data and examine how far from the mean flux in value each of them was. 
```{r,echo=T}
mu <- mean(diffs); mu
sigma <- sd(diffs); sigma

N <- length(diffs)
SDs <- numeric(N)
for (i in 1:N){
  SDs[i] <- (diffs[i]-mu)/sigma
}
head(SDs);head(diffs)
length(SDs)
SDs.data <- c(0,SDs[1:length(SDs)]); head(SDs.data) 
DJI <- data.frame(DJI,SDs.data)
idx <- which(abs(SDs.data) > 5); head(idx)
unusual <- DJI[idx,]; head(unusual) 
```
As we can see, there are 32 days in which the price flux for the Dow Jones was larger than 5 standard deviations away from the mean. To show just how bad of a fit the normal distribution is for our data consider the p-values for each of these events. 
```{r,echo=T}
N <- nrow(unusual)
pvals <- numeric(N)
for (i in 1:(N)) {
 pvals[i] <- pnorm(abs(unusual$SDs.data[i]*sigma), mean = mu, sd = sigma, lower.tail = FALSE)
}
head(pvals)
rare <- max(pvals);rare #2.303366e-07, which is pretty much 0. 

(1/rare)/365
```
If we interpret the p-value as the probability of an event taking place, and our events are measured in days, then a given p-value tells us the probability of seeing that extreme of an event on any given days (i.e. a p-value of 0.05 would correspond to an event that we'd "expect" to see once every 20 days, since it has a 1/20 chance of arising).In other words, if the DJI first differences followed a normal distribution,we would expect to see the least rare of these rare events once in 11,894.45 years, but from the data it is clear that these events are far more common than that.

#Surveying the impact of the White House on the Dow Jones
Now that we have seen that the data follows a model with infinite variance, which somewhat resemblesthat of a random walk, let us consider the hypothesis that political regimes have an impact on the market, here clearly represented by the Dow Jones industrial Average. We can do this by considering the impact of political party on the performance of the market. 

```{r,echo=T}
regime <- lm(DJI$diffs~DJI$Republican + DJI$Recession)
summary(regime)
```
At first glance, it looks like republican regimes tend to be negatively correlated with growth in the Dow.But let us look a little closer. Given the large standard error for the Republican coefficient, we cannot safely conclude that republican administrations correlate with losses in the Dow. (i.e. the p-value is .3, so we fail to reject the null hypothesis that this control variable has no impact on the response variable.)

Let's see if we can get a statistically signifficant result for any of the individual presidents:
```{r,echo=T}
GHWB <- DJI$Regime == "GHWB"
BC <- DJI$Regime == "BC"
GWB <- DJI$Regime == "GWB"
BO <- DJI$Regime == "BO"
DJT <- DJI$Regime == "DJT"

pres.binary <- data.frame(GHWB, BC, GWB, BO, DJT)

regress.data <- data.frame(DJI,pres.binary)

#since we omitted the Ronald Reagan variable, each of the coefficients for the various presidents 
#represents the incremental surplus or deficit in the DJIA that occurred during each of the other 
#presidents' terms
ind.pres <- lm(regress.data$diffs ~ regress.data$Recession + regress.data$GHWB + regress.data$BC  + regress.data$GWB + regress.data$BO + regress.data$DJT)
ind.pres
###These results are interesting becuase they seem out of line with the recent (2018-2019) rhetoric of the Donald 
###Trump administration's success in boosting the DJIA to new heights.
summary(ind.pres)
###here again, we find that no presidents' presence in the White House had a significant impact on the Dow.
```
These results are interesting becuase they seem out of line with the recent (2018-2019) rhetoric of the Donald Trump administration's success in boosting the DJIA to new heights. Here again, we find that no presidents' presence in the White House had a significant impact on the Dow.

Let us now exclude the days when 5 sigma + events took place, that way we'll keep only the data for "normal/typical" days.
```{r,echo=T}
regress.data.ne <- regress.data[-idx,]
ind.pres.2 <- lm(regress.data.ne$diffs ~ + regress.data.ne$Recession + regress.data.ne$GHWB + regress.data.ne$BC + regress.data.ne$GWB + regress.data.ne$BO + regress.data.ne$DJT)
summary(ind.pres.2)
```
These results seem to indicate that, excluding days with "extremely" events, and controlling for recessions (and nothing else), when we compare the performance of the DJIA during the previous 6 US presidents, the results indeed appear to be most favorable to Donald Trump, and least favorable to George W Bush. It appears that the negative coefficient related to GWB comes from the effects of the 2008 housing crisis, which took place in the final months of his second term. Additionally, we should note that the only statistically signifficant coefficient was that of Donald Trump, which had a p-value well below 0.01.

#Logistic Regression: Recessions and Results
Given the earlier results, it seems that recessions have a large impact on first differences in daily Open values for the Dow Jones Industrial Average.Let us inspect how economic recessions correlate with the performance of the DJIA. 
```{r,echo=T}
plot(DJI$diffs,DJI$Recession, xlim = c(-5000,5000))
MLL <- function(alpha, beta) {
  -sum( log( exp(alpha+beta*DJI$diffs)/(1+exp(alpha+beta*DJI$diffs)) )*DJI$Recession
        + log(1/(1+exp(alpha+beta*DJI$diffs)))*(1-DJI$Recession) )
}
#R has a function that will maximize this function of alpha and beta
#install.packages("stats4")   #needs to be run at most once
library(stats4)
results <- mle(MLL, start = list(alpha = 0, beta = 0)) #an initial guess is required
results@coef
curve( exp(results@coef[1] + results@coef[2]*x) / (1 + exp(results@coef[1] + results@coef[2]*x)),col = "blue", add=TRUE)
```
This is a fairly interesting result because its graph looks different than the normal logistic curve. Of course, this becomes obvious when one considers the nature of the regression, namely that recessions are events that are expected to correlate with negative values of first differences (i.e. price drops).In any case, this provides some evidence to support the hypothesis that negative fluxes in the Dow Jones correlate with economic recessions.
=======
Partial Variance
=======

#Cauchy Distribution
```{r}

```

#Bootstrap
```{r}
## Central Limit Theorem, Bootstrap and Empirical Cumulative Distribution
#
# Sources: 
# https://stats.stackexchange.com/questions/2504/test-for-finite-variance 
# Chihara and Hesterberg's Mathematical Statistics with Resampling
#
# If our sample is an independent and identically distributed realization 
# from a light-tailed distribution, the central limit theorem should hold
# even at smaller sample sizes. If we have a heavy-tailed distribution, 
# larger sample sizes will be needed to approximate the standard normal distribution.
# Use bootstrap resampling to demonstrate this.

## Bootstrap For A Single Population
par(mfrow = c(2,2)) # create 2x2 plot matrix
sampsize <- length(diffs) # starting sample size to draw from
N <- 100 # number of boostrap samples to run
n <-  100 # bootstrap sample size to draw
xlima <- -3 ; xlimb <- 3

# Perform N boostrap resamples of size n from sample X
meanY <- varY <- Z <- numeric(N)
plot(function(x) pnorm(x), xlim = c(xlima,xlimb), lwd = 5, main = "eCDF of Z from First Differences")
for (i in 1:N) {
  X <- diffs
  for (i in 1:N) {
    Y <- sample(X,n,replace = TRUE) # Resample
    meanY[i] <- mean(Y)
    varY[i] <- var(Y)
    Z[i] <- (mean(Y) - mean(X)) / (sd(Y)/sqrt(n))# Compute Z test statistic
  }
  lines(ecdf(Z), col = rgb(runif(1,0,1),runif(1,0,1),runif(1,0,1)), cex = .1)
}
meanY.diffs <- meanY
varY.diffs <- varY
Z.diffs <- Z
plot(function(x) pnorm(x), lwd = 3, add = TRUE)

# Perform N boostrap resamples of size n from sample Cauchy with interquartile parameters
meanY <- varY <- Z <- numeric(N)
plot(function(x) pnorm(x), xlim = c(xlima,xlimb), lwd = 5, main = "eCDF of Z from Cauchy (Interquartile)")
for (i in 1:N) {
  X <- rcauchy(sampsize, location = diffs.median, scale = diffs.hiq)
  for (i in 1:N) {
    Y <- sample(X,n,replace = TRUE) # Resample
    meanY[i] <- mean(Y)
    varY[i] <- var(Y)
    Z[i] <- (mean(Y) - mean(X)) / (sd(Y)/sqrt(n))# Compute Z test statistic
  }
  lines(ecdf(Z), col = rgb(runif(1,0,1),runif(1,0,1),runif(1,0,1)), cex = .1)
}
meanY.iqrCauchy <- meanY
varY.iqrCauchy <- varY
Z.iqrCauchy <- Z
plot(function(x) pnorm(x), lwd = 3, add = TRUE)

# Perform N boostrap resamples of size n from Cauchy with fitdist parameters
meanY <- varY <- Z <- numeric(N)
plot(function(x) pnorm(x), xlim = c(xlima,xlimb), lwd = 5, main = "eCDF of Z from Cauchy (FitDist)")
for (i in 1:N) {
  X <- rcauchy(sampsize, location = fit.diffs[1], scale = fit.diffs[2])
  for (i in 1:N) {
    Y <- sample(X,n,replace = TRUE) # Resample
    meanY[i] <- mean(Y)
    varY[i] <- var(Y)
    Z[i] <- (mean(Y) - mean(X)) / (sd(Y)/sqrt(n))# Compute Z test statistic
  }
  lines(ecdf(Z), col = rgb(runif(1,0,1),runif(1,0,1),runif(1,0,1)), cex = .1)
}
meanY.fdCauchy <- meanY
varY.fdCauchy <- varY
Z.fdCauchy <- Z
plot(function(x) pnorm(x), lwd = 3, add = TRUE)

# Perform N boostrap resamples of size n from normal distribution
meanY <- varY <- Z <- numeric(N)
plot(function(x) pnorm(x), xlim = c(xlima,xlimb), lwd = 5, main = "eCDF of Z from Normal")
for (i in 1:N) {
  X <- rnorm(sampsize, mean(diffs), sd(diffs)) 
  for (i in 1:N) {
    Y <- sample(X,n,replace = TRUE) # Resample
    meanY[i] <- mean(Y)
    varY[i] <- var(Y)
    Z[i] <- (mean(Y) - mean(X)) / (sd(Y)/sqrt(n))# Compute Z test statistic
  }
  lines(ecdf(Z), col = rgb(runif(1,0,1),runif(1,0,1),runif(1,0,1)), cex = .1)
}
meanY.norm <- meanY
varY.norm <- varY
Z.norm <- Z
plot(function(x) pnorm(x), lwd = 3, add = TRUE)

#########################################################
## Result: Empirical cumulative distribution for standardized random variable from
## bootstrap sampling distribution seems to indicate similarity between first differences
## and standard normal as well as dissimilarity between first differences and Cauchy. 
## Though there are indicates of wider dispersion in the eCDFs for the first differences
## than there are in the eCDFs for the normal, indicating the possibility of 
## significantly greater Kolmogorov-Smirnov test statistics. This requires testing. 


# Bootstrap sampling distribution of standardized sample observations
hist(Z.diffs, breaks = "fd", prob = TRUE) # looks normal
hist(Z.iqrCauchy, breaks = "fd", prob = TRUE) # somewhat normal, inconsistent center on iteration
hist(Z.fdCauchy, breaks = "fd", prob = TRUE) # somewhat normal, inconsistent center on iteration
hist(Z.norm, breaks = "fd", prob = TRUE) # looks normal
# Result: Central Limit Theorem explains approximation of standard normal for larger sample sizes. 
# However, due to infinite variance of Cauchy distribution, extreme values from heavy tails
# require very large sample size to approximate standard normal distribution. 
# Cauchy has inconsistent center, shape and spread on different iterations. 

# Bootstrap sampling distribution of sample variances
hist(varY.diffs, breaks = "fd", prob = TRUE) # somewhat normal with somewhat heavy right tail
hist(varY.iqrCauchy, breaks = "fd", prob = TRUE) # heavy right tail
hist(varY.fdCauchy, breaks = "fd", prob = TRUE) # heavy right tail
hist(varY.norm, breaks = "fd", prob = TRUE) # looks normal
# Result: Variance for first difference somewhat resembles shape and spread of Cauchy variance, not normal.

# Bootstrap sampling distribution of sample means
hist(meanY.diffs, breaks = "fd", prob = TRUE) # looks normal
hist(meanY.iqrCauchy, breaks = "fd", prob = TRUE) # looks like stable distribution with large variance
hist(meanY.fdCauchy, breaks = "fd", prob = TRUE) # looks like stable distribution with large variance
hist(meanY.norm, breaks = "fd", prob = TRUE) # looks normal
# Result: Mean for first difference resembles shape and spread of normal sample mean, not Cauhcy.

## Overall Result: Our first differences data may lie somewhere between Gaussian and Cauchy 
## on the parameter scale for stable distributions. However, the data is a sample from 
## an underlying population, so the limited accessibilility to a sample size of only 8857 
## limits the capture of extreme values from possibly heavy right tails. 
```


#Partial Variance
>>>>>>> 5b5c0749e494546bd316b6eb7560cbf5ac1ca52c
```{r}
# Partial Variance to test for convergence of variance
N <- length(Open) - 1; 
variances.normal <- variances.cauchy <- variances.Open <- variances.diffs <- numeric(N)
sample.normal <- rnorm(N + 1) ; sample.cauchy <- rcauchy(N + 1)
Open <- DJI$Open ; diffs.Open <- DJI$diffs
index <- 1:N
for (i in 2:(N + 1)) {
  variances.normal[i - 1] <- var(sample.normal[1:i])
  variances.cauchy[i - 1] <- var(sample.cauchy[1:i])
  variances.Open[i - 1] <- var(Open[1:i])
  variances.diffs[i - 1] <- var(diffs.Open[1:i])
}
variances.diffs <- variances.diffs[-1]
par(mfrow = c(2,2)) # create 2x2 plot matrix
plot(index,variances.normal, type = "l", col = "steelblue", log = "x", ylab = "Normal Variance", xlab = "Sample Size") # converges
plot(index,variances.cauchy, type = "l", col = "firebrick", log = "xy",ylab = "Cauchy Variance", xlab = "Sample Size") # diverges jagged
plot(index,variances.Open, type = "l", col = "yellowgreen", log = "xy",ylab = "Open Variance", xlab = "Sample Size") # diverges
plot(head(index,-1),variances.diffs, type = "l", col = "slategray", log = "xy", ylab = "Firs Diff. Variance", xlab = "Sample Size") # diverges
par(mfrow = c(1,1)) # revert to 1x1 plot matrix
summary(variances.normal) # data is centered closely around mean and median
summary(variances.cauchy) # seems to be large spread
summary(variances.Open) # extremely large spread
summary(variances.diffs) # spread is larger than it is for Cauchy but less than Open prices
### Result: As index increases, partial variance converges for normal distribution,
### but it diverges in jagged jumps for Cauchy distribution and
### in smoother curves for both Open values and first differences. 
### This indicates that our data may have undefined or infinite variance. 
```


#Divergent Integrals, Divergent Variance
```{r}
## Divergent Integration For Calculating Variance

#Now let us try to show that, if our data is modeled by a Cauchy distribution with the location and 
#scale parameters above, that it does indeed have infinite variance. 
#To do that, we'll need to define functions that will describe the integrands that will allow us to
#use the tail-integral theorem. First, we'll define the integrand that will give us E(X).
#Since the underlying distribution can be modeled by a Cauchy distribution, we will use 
#dcauchy with the right parameters as our mu_x:
integrand <- function(x) dcauchy(x, location = fit.diffs[1], scale = fit.diffs[2])*x
#and now we have E(X):
exp.x <- integrate(f = integrand, lower = -Inf, upper = Inf)$value; exp.x 

#In the same manner, we can try to calculate E(X^2) so that we can get Var = E(X^2) - E(X)^2
integrand2 <- function(x) dcauchy(x, location = fit.diffs[1], scale = fit.diffs[2])*x^2
#And E(X^2)
exp.x2 <- integrate(f = integrand2, lower = -Inf, upper = Inf)$value; exp.x2 
#And it appears that the integral is divergent! This means that 
#Var = E(X^2) - E(X)^2 also diverges, and thus Var = Inf!
```
>>>>>>> ab7de49cc1c3eccf96c2ecf068143a939a62150a

#Stable Distribution
```{r}

```


#The Impact of Political Regimes - Hypothesis Testing With Permutation Test 
```{r, echo=TRUE}
RepAvg <- sum(DJI$diffs*(DJI$Republican == TRUE))/sum(DJI$Republican == TRUE) ; RepAvg
DemAvg <- sum(DJI$diffs*(DJI$Republican == FALSE))/sum(DJI$Republican == FALSE) ; DemAvg
Obs <-  DemAvg - RepAvg; Obs
#
N <- 10^4 #number of simulations
result.Combined <- numeric(N) #this is the vector that will store our simulated differences
for (i in 1:N) {
  Rep <- sample(DJI$Republican) #This is our permuted party column
  RepMu <- sum(DJI$diffs*(Rep == TRUE))/sum(Rep == TRUE) ; RepMu
  DemMu <- sum(DJI$diffs*(Rep == FALSE))/sum(Rep == FALSE) ; DemMu
  result.Combined[i] <- DemMu - RepMu
  }
mean(result.Combined) #inspecting that these are indeed close to zero
hist(result.Combined, breaks = "FD", probability = TRUE, col = "steelblue")
abline(v = Obs, col = "red")
pvalue <-  (sum(result.Combined >= Obs) + 1)/(N + 1) ; pvalue
```
Giving a p-value of 2.87% chance that this extreme of an observed difference would arise by chance, so it appears that the DJI performed better during democratic regimes, a result that is statistically signifficant.

# Hypothesis Testing: Contingency table with chi-square test for political party and recession. 
```{r,echo=TRUE}
p <- sum(DJI$Recession)/length(DJI$Recession) # 17.67% of observations are in recession years
obs.tbl <- table(DJI$Republican,DJI$Recession)# Republican has more Recession
colnames(obs.tbl) <- c("Expansion", "Recession")
rownames(obs.tbl) <- c("Democrat", "Republican")
exp.tbl <- outer(rowSums(obs.tbl), colSums(obs.tbl))/sum(obs.tbl)
colnames(exp.tbl) <- c("Expansion", "Recession")
rownames(exp.tbl) <- c("Democrat", "Republican")
obs.tbl ; exp.tbl
chisq.test(DJI$Republican,DJI$Recession)
```
As we can see from this contingency table, Republicans had more days in office during recessions, but 
they also had more days in office during expansions.
The result of the chi-square test is a p-value is less than 2.2e-16, far below our .05 threshold, so there would be a verysmall chance that the observed contingency table would arise by chance. Thus, the observations provide sufficient evidence to reject the null hypothesis that Republican and Democratic regimes are equally likely to be associated with recession years from 1985 to early 2020. 

Let us try running this as chi-square test of contingency table including all regimes:
```{r,echo=TRUE}
obs.tbl <- table(DJI$Recession, DJI$Regime); rownames(obs.tbl) <- c("Expansion", "Recession"); obs.tbl #GWB had the most recession days
exp.tbl <- outer(rowSums(obs.tbl), colSums(obs.tbl))/sum(obs.tbl); rownames(exp.tbl) <- c("Expansion", "Recession"); exp.tbl
#This table allows us to see a breakdown of how long each president was in office in terms of recessions and 
#expansions
chisqvalue <- sum((obs.tbl - exp.tbl)^2/exp.tbl)

```
Here we can see that GWB had the most recession days. We can also see how long each president was in office in terms of recessions and how many recession days we would've expected, if they occurred evenly through the years.
```{r,echo=T}
P.Value <- pchisq(chisqvalue, df = (2 - 1) * (6 - 1), lower.tail = FALSE); P.Value
```
And we get a p-value of zero, thus we reject the null hypothesis that recession years are equaly likely to arise across regimes. 

Lastly, we can run this analysis as chi-square test specific to each regime with p the observed probabilty of recession:
```{r, echo=T}
q <- 1 - p; q # 0.8233235 probability of not being in a recession
prob <- (DJI$Recession*p + (!DJI$Recession)*q) / sum(DJI$Recession*p + (!DJI$Recession)*q) 
min(prob) ; max(prob) ; sum(prob) 
for (i in unique(DJI$Regime)) {
  print(chisq.test(DJI$Recession, DJI$Regime == i, p = prob))
}
```
Null hypothesis is that each regime has the observed probability p of recession across regimes. 
Note: There could exist carryover/lingering effects of recession or otherwise from one regime to the next.
Each p-value is less than 2.2e-16, far below the .05 threshold. This indicates that no individual regime is equally likely to be associated with recessions from the years 1985 to early 2020. 


