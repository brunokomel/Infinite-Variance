---
title: "Report"
author: "Derl Clausen, Bruno Komel, Rakeen Tanvir"
date: "5/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<<<<<<< HEAD
#Outline
Exploratory Data Analysis
Pareto Distribution
Linear and Logistic Regression
Fourier Analysis
Random Walk
Fractal Analysis
Cauchy Distribution
Bootstrap
Partial Variance
Divergent Integration
Stable Distribution
Kolmogorov-Smirnov Test
Chi-square Test on Varying Time Scales
The Impact of Political Regimes - Hypothesis Testing With Permutation Test 
Hypothesis Testing: Contingency table with chi-square test
=======


#Cauchy Distribution
```{r}

```

#Bootstrap
```{r}
## Central Limit Theorem, Bootstrap and Empirical Cumulative Distribution
#
# Sources: 
# https://stats.stackexchange.com/questions/2504/test-for-finite-variance 
# Chihara and Hesterberg's Mathematical Statistics with Resampling
#
# If our sample is an independent and identically distributed realization 
# from a light-tailed distribution, the central limit theorem should hold
# even at smaller sample sizes. If we have a heavy-tailed distribution, 
# larger sample sizes will be needed to approximate the standard normal distribution.
# Use bootstrap resampling to demonstrate this.

## Bootstrap For A Single Population
par(mfrow = c(2,2)) # create 2x2 plot matrix
sampsize <- length(diffs) # starting sample size to draw from
N <- 100 # number of boostrap samples to run
n <-  100 # bootstrap sample size to draw
xlima <- -3 ; xlimb <- 3

# Perform N boostrap resamples of size n from sample X
meanY <- varY <- Z <- numeric(N)
plot(function(x) pnorm(x), xlim = c(xlima,xlimb), lwd = 5, main = "eCDF of Z from First Differences")
for (i in 1:N) {
  X <- diffs
  for (i in 1:N) {
    Y <- sample(X,n,replace = TRUE) # Resample
    meanY[i] <- mean(Y)
    varY[i] <- var(Y)
    Z[i] <- (mean(Y) - mean(X)) / (sd(Y)/sqrt(n))# Compute Z test statistic
  }
  lines(ecdf(Z), col = rgb(runif(1,0,1),runif(1,0,1),runif(1,0,1)), cex = .1)
}
meanY.diffs <- meanY
varY.diffs <- varY
Z.diffs <- Z
plot(function(x) pnorm(x), lwd = 3, add = TRUE)

# Perform N boostrap resamples of size n from sample Cauchy with interquartile parameters
meanY <- varY <- Z <- numeric(N)
plot(function(x) pnorm(x), xlim = c(xlima,xlimb), lwd = 5, main = "eCDF of Z from Cauchy (Interquartile)")
for (i in 1:N) {
  X <- rcauchy(sampsize, location = diffs.median, scale = diffs.hiq)
  for (i in 1:N) {
    Y <- sample(X,n,replace = TRUE) # Resample
    meanY[i] <- mean(Y)
    varY[i] <- var(Y)
    Z[i] <- (mean(Y) - mean(X)) / (sd(Y)/sqrt(n))# Compute Z test statistic
  }
  lines(ecdf(Z), col = rgb(runif(1,0,1),runif(1,0,1),runif(1,0,1)), cex = .1)
}
meanY.iqrCauchy <- meanY
varY.iqrCauchy <- varY
Z.iqrCauchy <- Z
plot(function(x) pnorm(x), lwd = 3, add = TRUE)

# Perform N boostrap resamples of size n from Cauchy with fitdist parameters
meanY <- varY <- Z <- numeric(N)
plot(function(x) pnorm(x), xlim = c(xlima,xlimb), lwd = 5, main = "eCDF of Z from Cauchy (FitDist)")
for (i in 1:N) {
  X <- rcauchy(sampsize, location = fit.diffs[1], scale = fit.diffs[2])
  for (i in 1:N) {
    Y <- sample(X,n,replace = TRUE) # Resample
    meanY[i] <- mean(Y)
    varY[i] <- var(Y)
    Z[i] <- (mean(Y) - mean(X)) / (sd(Y)/sqrt(n))# Compute Z test statistic
  }
  lines(ecdf(Z), col = rgb(runif(1,0,1),runif(1,0,1),runif(1,0,1)), cex = .1)
}
meanY.fdCauchy <- meanY
varY.fdCauchy <- varY
Z.fdCauchy <- Z
plot(function(x) pnorm(x), lwd = 3, add = TRUE)

# Perform N boostrap resamples of size n from normal distribution
meanY <- varY <- Z <- numeric(N)
plot(function(x) pnorm(x), xlim = c(xlima,xlimb), lwd = 5, main = "eCDF of Z from Normal")
for (i in 1:N) {
  X <- rnorm(sampsize, mean(diffs), sd(diffs)) 
  for (i in 1:N) {
    Y <- sample(X,n,replace = TRUE) # Resample
    meanY[i] <- mean(Y)
    varY[i] <- var(Y)
    Z[i] <- (mean(Y) - mean(X)) / (sd(Y)/sqrt(n))# Compute Z test statistic
  }
  lines(ecdf(Z), col = rgb(runif(1,0,1),runif(1,0,1),runif(1,0,1)), cex = .1)
}
meanY.norm <- meanY
varY.norm <- varY
Z.norm <- Z
plot(function(x) pnorm(x), lwd = 3, add = TRUE)

#########################################################
## Result: Empirical cumulative distribution for standardized random variable from
## bootstrap sampling distribution seems to indicate similarity between first differences
## and standard normal as well as dissimilarity between first differences and Cauchy. 
## Though there are indicates of wider dispersion in the eCDFs for the first differences
## than there are in the eCDFs for the normal, indicating the possibility of 
## significantly greater Kolmogorov-Smirnov test statistics. This requires testing. 


# Bootstrap sampling distribution of standardized sample observations
hist(Z.diffs, breaks = "fd", prob = TRUE) # looks normal
hist(Z.iqrCauchy, breaks = "fd", prob = TRUE) # somewhat normal, inconsistent center on iteration
hist(Z.fdCauchy, breaks = "fd", prob = TRUE) # somewhat normal, inconsistent center on iteration
hist(Z.norm, breaks = "fd", prob = TRUE) # looks normal
# Result: Central Limit Theorem explains approximation of standard normal for larger sample sizes. 
# However, due to infinite variance of Cauchy distribution, extreme values from heavy tails
# require very large sample size to approximate standard normal distribution. 
# Cauchy has inconsistent center, shape and spread on different iterations. 

# Bootstrap sampling distribution of sample variances
hist(varY.diffs, breaks = "fd", prob = TRUE) # somewhat normal with somewhat heavy right tail
hist(varY.iqrCauchy, breaks = "fd", prob = TRUE) # heavy right tail
hist(varY.fdCauchy, breaks = "fd", prob = TRUE) # heavy right tail
hist(varY.norm, breaks = "fd", prob = TRUE) # looks normal
# Result: Variance for first difference somewhat resembles shape and spread of Cauchy variance, not normal.

# Bootstrap sampling distribution of sample means
hist(meanY.diffs, breaks = "fd", prob = TRUE) # looks normal
hist(meanY.iqrCauchy, breaks = "fd", prob = TRUE) # looks like stable distribution with large variance
hist(meanY.fdCauchy, breaks = "fd", prob = TRUE) # looks like stable distribution with large variance
hist(meanY.norm, breaks = "fd", prob = TRUE) # looks normal
# Result: Mean for first difference resembles shape and spread of normal sample mean, not Cauhcy.

## Overall Result: Our first differences data may lie somewhere between Gaussian and Cauchy 
## on the parameter scale for stable distributions. However, the data is a sample from 
## an underlying population, so the limited accessibilility to a sample size of only 8857 
## limits the capture of extreme values from possibly heavy right tails. 
```


#Partial Variance
```{r}
# Partial Variance to test for convergence of variance
N <- length(Open) - 1; 
variances.normal <- variances.cauchy <- variances.Open <- variances.diffs <- numeric(N)
sample.normal <- rnorm(N + 1) ; sample.cauchy <- rcauchy(N + 1)
Open <- DJI$Open ; diffs.Open <- DJI$diffs
index <- 1:N
for (i in 2:(N + 1)) {
  variances.normal[i - 1] <- var(sample.normal[1:i])
  variances.cauchy[i - 1] <- var(sample.cauchy[1:i])
  variances.Open[i - 1] <- var(Open[1:i])
  variances.diffs[i - 1] <- var(diffs.Open[1:i])
}
variances.diffs <- variances.diffs[-1]
par(mfrow = c(2,2)) # create 2x2 plot matrix
plot(index,variances.normal, type = "l", col = "steelblue", log = "x", ylab = "Normal Variance", xlab = "Sample Size") # converges
plot(index,variances.cauchy, type = "l", col = "firebrick", log = "xy",ylab = "Cauchy Variance", xlab = "Sample Size") # diverges jagged
plot(index,variances.Open, type = "l", col = "yellowgreen", log = "xy",ylab = "Open Variance", xlab = "Sample Size") # diverges
plot(head(index,-1),variances.diffs, type = "l", col = "slategray", log = "xy", ylab = "Firs Diff. Variance", xlab = "Sample Size") # diverges
par(mfrow = c(1,1)) # revert to 1x1 plot matrix
summary(variances.normal) # data is centered closely around mean and median
summary(variances.cauchy) # seems to be large spread
summary(variances.Open) # extremely large spread
summary(variances.diffs) # spread is larger than it is for Cauchy but less than Open prices
### Result: As index increases, partial variance converges for normal distribution,
### but it diverges in jagged jumps for Cauchy distribution and
### in smoother curves for both Open values and first differences. 
### This indicates that our data may have undefined or infinite variance. 
```


#Divergent Integrals, Divergent Variance
```{r}
## Divergent Integration For Calculating Variance

#Now let us try to show that, if our data is modeled by a Cauchy distribution with the location and 
#scale parameters above, that it does indeed have infinite variance. 
#To do that, we'll need to define functions that will describe the integrands that will allow us to
#use the tail-integral theorem. First, we'll define the integrand that will give us E(X).
#Since the underlying distribution can be modeled by a Cauchy distribution, we will use 
#dcauchy with the right parameters as our mu_x:
integrand <- function(x) dcauchy(x, location = fit.diffs[1], scale = fit.diffs[2])*x
#and now we have E(X):
exp.x <- integrate(f = integrand, lower = -Inf, upper = Inf)$value; exp.x 

#In the same manner, we can try to calculate E(X^2) so that we can get Var = E(X^2) - E(X)^2
integrand2 <- function(x) dcauchy(x, location = fit.diffs[1], scale = fit.diffs[2])*x^2
#And E(X^2)
exp.x2 <- integrate(f = integrand2, lower = -Inf, upper = Inf)$value; exp.x2 
#And it appears that the integral is divergent! This means that 
#Var = E(X^2) - E(X)^2 also diverges, and thus Var = Inf!
```

#Stable Distribution
```{r}

```


#The Impact of Political Regimes - Hypothesis Testing With Permutation Test 
```{r, echo=TRUE}
RepAvg <- sum(DJI$diffs*(DJI$Republican == TRUE))/sum(DJI$Republican == TRUE) ; RepAvg
DemAvg <- sum(DJI$diffs*(DJI$Republican == FALSE))/sum(DJI$Republican == FALSE) ; DemAvg
Obs <-  DemAvg - RepAvg; Obs
#
N <- 10^4 #number of simulations
result.Combined <- numeric(N) #this is the vector that will store our simulated differences
for (i in 1:N) {
  Rep <- sample(DJI$Republican) #This is our permuted party column
  RepMu <- sum(DJI$diffs*(Rep == TRUE))/sum(Rep == TRUE) ; RepMu
  DemMu <- sum(DJI$diffs*(Rep == FALSE))/sum(Rep == FALSE) ; DemMu
  result.Combined[i] <- DemMu - RepMu
  }
mean(result.Combined) #inspecting that these are indeed close to zero
hist(result.Combined, breaks = "FD", probability = TRUE, col = "steelblue")
abline(v = Obs, col = "red")
pvalue <-  (sum(result.Combined >= Obs) + 1)/(N + 1) ; pvalue
```
Giving a p-value of 2.87% chance that this extreme of an observed difference would arise by chance, so it appears that the DJI performed better during democratic regimes, a result that is statistically signifficant.

# Hypothesis Testing: Contingency table with chi-square test for political party and recession. 
```{r,echo=TRUE}
p <- sum(DJI$Recession)/length(DJI$Recession) # 17.67% of observations are in recession years
obs.tbl <- table(DJI$Republican,DJI$Recession)# Republican has more Recession
colnames(obs.tbl) <- c("Expansion", "Recession")
rownames(obs.tbl) <- c("Democrat", "Republican")
exp.tbl <- outer(rowSums(obs.tbl), colSums(obs.tbl))/sum(obs.tbl)
colnames(exp.tbl) <- c("Expansion", "Recession")
rownames(exp.tbl) <- c("Democrat", "Republican")
obs.tbl ; exp.tbl
chisq.test(DJI$Republican,DJI$Recession)
```
As we can see from this contingency table, Republicans had more days in office during recessions, but 
they also had more days in office during expansions.
The result of the chi-square test is a p-value is less than 2.2e-16, far below our .05 threshold, so there would be a verysmall chance that the observed contingency table would arise by chance. Thus, the observations provide sufficient evidence to reject the null hypothesis that Republican and Democratic regimes are equally likely to be associated with recession years from 1985 to early 2020. 

Let us try running this as chi-square test of contingency table including all regimes:
```{r,echo=TRUE}
obs.tbl <- table(DJI$Recession, DJI$Regime); rownames(obs.tbl) <- c("Expansion", "Recession"); obs.tbl #GWB had the most recession days
exp.tbl <- outer(rowSums(obs.tbl), colSums(obs.tbl))/sum(obs.tbl); rownames(exp.tbl) <- c("Expansion", "Recession"); exp.tbl
#This table allows us to see a breakdown of how long each president was in office in terms of recessions and 
#expansions
chisqvalue <- sum((obs.tbl - exp.tbl)^2/exp.tbl)

```
Here we can see that GWB had the most recession days. We can also see how long each president was in office in terms of recessions and how many recession days we would've expected, if they occurred evenly through the years.
```{r,echo=T}
P.Value <- pchisq(chisqvalue, df = (2 - 1) * (6 - 1), lower.tail = FALSE); P.Value
```
And we get a p-value of zero, thus we reject the null hypothesis that recession years are equaly likely to arise across regimes. 

Lastly, we can run this analysis as chi-square test specific to each regime with p the observed probabilty of recession:
```{r, echo=T}
q <- 1 - p; q # 0.8233235 probability of not being in a recession
prob <- (DJI$Recession*p + (!DJI$Recession)*q) / sum(DJI$Recession*p + (!DJI$Recession)*q) 
min(prob) ; max(prob) ; sum(prob) 
for (i in unique(DJI$Regime)) {
  print(chisq.test(DJI$Recession, DJI$Regime == i, p = prob))
}
```
Null hypothesis is that each regime has the observed probability p of recession across regimes. 
Note: There could exist carryover/lingering effects of recession or otherwise from one regime to the next.
Each p-value is less than 2.2e-16, far below the .05 threshold. This indicates that no individual regime is equally likely to be associated with recessions from the years 1985 to early 2020. 


